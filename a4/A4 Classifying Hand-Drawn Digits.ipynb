{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucas Wilson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4 Classifying Hand-Drawn Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "For this assignment you will apply  a neural network classifier to the problem of classifying handwritten digits.  You will use the provided code of the classifier that is implemented using `pytorch`.  You will experiment with various parameter values and describe your results.\n",
    "\n",
    "If you are planning to use `pytorch` on the workstations installed in the Department of Computer Science, you must execute this command\n",
    "\n",
    "`export PYTHONPATH=$PYTHONPATH:/usr/local/anaconda/lib/python3.6/site-packages/`\n",
    "\n",
    "A better solution is to add it to your startup script, such as `.bashrc`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided Code and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [A4.zip](http://www.cs.colostate.edu/~anderson/cs445/notebooks/A4.zip) and unzip it.  You should have two files, `neuralnetworks_pytorch.py` and `mnist.pkl.gz`. \n",
    "\n",
    "Load the data using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 1, 28, 28), (50000,), (10000, 1, 28, 28), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "Xtrain = train_set[0].reshape((-1, 1, 28, 28))\n",
    "Ttrain = train_set[1]\n",
    "Xtest = test_set[0].reshape((-1, 1, 28, 28))\n",
    "Ttest = test_set[1]\n",
    "\n",
    "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dimension of `Xtrain` and `Xtest` is 1, representing the number of values, or channels, in each pixel.  These images are gray scale so have just one intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the provided neural network code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralnetworks_pytorch as nn\n",
    "from neuralnetworks_pytorch import NeuralNetworkClassifier_Pytorch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the class `NeuralNetworkClassifier_Pytorch`.  The constructor for this class accepts the arguments\n",
    "\n",
    "  * `n_inputs`: (int) number of input components, which is the number of channels for a convolutional net, or the total number of pixels for a fully connected network\n",
    "  * `n_hiddens_by_layer`: (list of ints) number of units in each hidden layer\n",
    "  * `n_outputs`: (int) number of classes in the data\n",
    "  * `relu`: (boolean, default False) if True, relu is used as the activation function. If False, the activation function is tanh\n",
    "  * `gpu`: (boolean, default False) If True and this machine has a compatible GPU, run the network on the GPU\n",
    "  * `n_conv_layers`: (int, default 0) 0 to create all layers as fully connected, else create this many convolutional layers as the initial layers in the network\n",
    "  * `windows`: (list of ints, default [ ]) if all layers are fully connected, this should be empty. If network contains convolutional layers this must be a list of length equal to `n_conv_layers`, with an int for each layer specifying the height and width of the convolution window\n",
    "  * `strides`: (list of ints, default [ ]) if all layers are fully connected, this should be empty. If network contains convolutional layers this must be a list of length equal to `n_conv_layers`, with an int for each layer specifying the horizontal and vertical stride ofthe convolution window\n",
    "  * `input_height_width`: (int or None, default value) height and width of input image but only needed for convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then train this neural network using the `train` function\n",
    "\n",
    "  * `Xtrain`: (np.ndarray of floats) training samples along first dimension\n",
    "  * `Ttrain`: (np.ndarray of longs) one-dimensional vector of integers indicating class of each training sample\n",
    "  * `Xtest`: (np.ndarray of floats) testing samples along first dimension\n",
    "  * `Ttest`: (np.ndarray of longs) one-dimensional vector of integers indicating class of each testing sample,\n",
    "  * `n_iterations`: (int) number of optimization steps, sometimes called epochs\n",
    "  * `batch_size`: (int) number of samples in each batch to calculate gradient for and update all weights\n",
    "  * `learning_rate`: (float) factor multiplying gradient to determine step size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a neural net is created, with a line like\n",
    "\n",
    "     nnet = nn.NeuralNetworkClassifier_Pytorch(1, [10, 20, 5], 10, \n",
    "               n_conv_layers=2, windows=[5, 7], strides=[1, 2], input_height_width=28)\n",
    "\n",
    "it can be trained with a line like\n",
    "\n",
    "     nnet.train(Xtrain, Ttrain, Xtest, Ttest, 200, 100, 0.001)\n",
    "     \n",
    "and predictions are made with\n",
    "\n",
    "     classes, probs = nnet.use(Xtrain)\n",
    "\n",
    "where `classes` are the predicted classes for each sample and `prob` is the probability of each class for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the precent of predicted classes that are correct, use the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_correct(actual, predicted):\n",
    "    return 100 * np.mean(actual == predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Section 1: Fully-Connected Networks (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (15 points) Using `batch_size` of 100, `learning_rate` of 0.001 and `n_hiddens_by_layer` of `[20, 20, 20]`, try a variety of `n_iterations` values and plot the percent of testing data correctly classified versus `n_iterations`.\n",
    "\n",
    "2. (15 points) Using the best value of `n_iterations`, try at least five different values of `n_hiddens_by_layer` and plot the percent of testing data correctly classified.\n",
    "\n",
    "3. (10 points) Describe what you see in your plots with at least two sentences for each plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Best n_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using n_iterations: 1\n",
      "NeuralNetworkClassifier_Pytorch created on cpu\n",
      "Iteration 1, cost = 0.8697, acc = 90.82\n",
      "Using n_iterations: 10\n",
      "NeuralNetworkClassifier_Pytorch created on cpu\n",
      "Iteration 1, cost = 0.8258, acc = 91.34\n",
      "Iteration 2, cost = 0.2951, acc = 93.18\n",
      "Iteration 3, cost = 0.2270, acc = 94.19\n",
      "Iteration 4, cost = 0.1930, acc = 94.60\n",
      "Iteration 5, cost = 0.1706, acc = 95.00\n",
      "Iteration 6, cost = 0.1543, acc = 95.10\n",
      "Iteration 7, cost = 0.1414, acc = 95.24\n",
      "Iteration 8, cost = 0.1309, acc = 95.38\n",
      "Iteration 9, cost = 0.1222, acc = 95.49\n",
      "Iteration 10, cost = 0.1149, acc = 95.48\n",
      "Using n_iterations: 20\n",
      "NeuralNetworkClassifier_Pytorch created on cpu\n",
      "Iteration 2, cost = 0.3071, acc = 92.58\n",
      "Iteration 4, cost = 0.1918, acc = 94.39\n",
      "Iteration 6, cost = 0.1504, acc = 94.78\n",
      "Iteration 8, cost = 0.1265, acc = 94.91\n",
      "Iteration 10, cost = 0.1108, acc = 95.01\n",
      "Iteration 12, cost = 0.0994, acc = 95.12\n",
      "Iteration 14, cost = 0.0902, acc = 95.13\n",
      "Iteration 16, cost = 0.0825, acc = 95.29\n",
      "Iteration 18, cost = 0.0758, acc = 95.27\n",
      "Iteration 20, cost = 0.0698, acc = 95.23\n",
      "Using n_iterations: 50\n",
      "NeuralNetworkClassifier_Pytorch created on cpu\n",
      "Iteration 5, cost = 0.1670, acc = 94.72\n",
      "Iteration 10, cost = 0.1134, acc = 95.46\n",
      "Iteration 15, cost = 0.0879, acc = 95.48\n",
      "Iteration 20, cost = 0.0710, acc = 95.34\n",
      "Iteration 25, cost = 0.0586, acc = 95.24\n"
     ]
    }
   ],
   "source": [
    "def run1(n_iterations):\n",
    "    nnet = NeuralNetworkClassifier_Pytorch(\n",
    "        n_inputs=1*28*28,\n",
    "        n_hiddens_by_layer=[20, 20, 20],\n",
    "        n_outputs=10,\n",
    "        #relu=,\n",
    "        #gpu=,\n",
    "        #n_conv_layers=,\n",
    "        #windows=,\n",
    "        #strides=,\n",
    "        #input_height_width=,\n",
    "    )\n",
    "    nnet.train(\n",
    "        Xtrain=Xtrain,\n",
    "        Ttrain=Ttrain,\n",
    "        Xtest=Xtest,\n",
    "        Ttest=Ttest,\n",
    "        n_iterations=n_iterations,\n",
    "        batch_size=100,\n",
    "        learning_rate=0.001,\n",
    "    )\n",
    "    return nnet\n",
    "\n",
    "n_iterations_test_values = [1, 10, 20, 50, 60, 70, 80, 100, 150, 200, 300, 500, 750, 1000]\n",
    "num_it_test_accuracy = []\n",
    "for n_iterations in n_iterations_test_values:\n",
    "    print('Using n_iterations:', n_iterations)\n",
    "    nnet = run1(n_iterations)\n",
    "    Ytest, _, _ = nnet.use(Xtest)\n",
    "    num_correct = np.sum(Ytest == Ttest)\n",
    "    num_it_test_accuracy.append(num_correct / Ytest.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Section 1, Part 1, Test Percentage Correct vs Num Iterations')\n",
    "plt.xlabel('Num Iterations')\n",
    "plt.ylabel('Test Percentage Correct')\n",
    "plt.xticks(n_iterations_test_values, n_iterations_test_values)\n",
    "plt.plot(n_iterations_test_values, num_it_test_accuracy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.argmax(num_it_test_accuracy)\n",
    "best_n_iterations = n_iterations_test_values[best_index]\n",
    "print('best n_iterations:', best_n_iterations, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commentary on Graph: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Best Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run2(n_hiddens_by_layer):\n",
    "    nnet = NeuralNetworkClassifier_Pytorch(\n",
    "        n_inputs=1*28*28,\n",
    "        n_hiddens_by_layer=n_hiddens_by_layer,\n",
    "        n_outputs=10,\n",
    "        #relu=,\n",
    "        #gpu=,\n",
    "        #n_conv_layers=,\n",
    "        #windows=,\n",
    "        #strides=,\n",
    "        #input_height_width=,\n",
    "    )\n",
    "    nnet.train(\n",
    "        Xtrain=Xtrain,\n",
    "        Ttrain=Ttrain,\n",
    "        Xtest=Xtest,\n",
    "        Ttest=Ttest,\n",
    "        n_iterations=best_n_iterations,\n",
    "        batch_size=100,\n",
    "        learning_rate=0.001,\n",
    "    )\n",
    "    return nnet\n",
    "\n",
    "\n",
    "n_hiddens_by_layer_test_values = [[nu] * nl for nu in [1, 5, 10, 20, 50] for nl in [1, 2, 3, 4, 5]]\n",
    "shape_test_accuracy = []\n",
    "\n",
    "for n_hiddens_by_layer in n_hiddens_by_layer_test_values:\n",
    "    print('Using nnet shape:', n_hiddens_by_layer)\n",
    "    nnet = run2(n_hiddens_by_layer)\n",
    "    Ytest, _, _ = nnet.use(Xtest)\n",
    "    num_correct = np.sum(Ytest == Ttest)\n",
    "    shape_test_accuracy.append(num_correct / Ytest.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(shape_test_accuracy, 'o-')\n",
    "plt.title(\"Test RMSE vs Hidden Layers Structure\")\n",
    "plt.ylabel('Test RMSE')\n",
    "plt.xticks(range(len(shape_test_accuracy)), n_hiddens_by_layer_test_values, rotation=30, horizontalalignment='right')\n",
    "plt.xlabel('Hidden Layers Structure')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.argmax(shape_test_accuracy)\n",
    "best_shape = shape_test_accuracy[best_index]\n",
    "print('best shape:', n_hiddens_by_layer_test_values[best_index], \"with\", best_shape, \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Commentary on graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Section 2: Convolutional Networks (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (15 points) Using `batch_size` of 100, `learning_rate` of 0.001, `n_hiddens_by_layer` of `[20, 20, 20]`, `n_iterations` of 10, `n_conv_layers` of 2, and try several values of `windows` and of `strides`, and plot the percent of testing data correctly classified versus `windows` and `strides`.\n",
    "\n",
    "2. (15 points) Try several more variations of `n_hiddens_by_layer`, `n_iterations`, `n_conv_layers`, `windows` and `strides`. \n",
    "\n",
    "3. (10 points) Describe what you see in your plots in 1., and the variations you see in 2. with at least two sentences for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run21(windows, strides):\n",
    "    nnet = NeuralNetworkClassifier_Pytorch(\n",
    "        n_inputs=1,\n",
    "        n_hiddens_by_layer=[20, 20, 20],\n",
    "        n_outputs=10,\n",
    "        #relu=,\n",
    "        #gpu=,\n",
    "        n_conv_layers=2,\n",
    "        windows=windows,\n",
    "        strides=strides,\n",
    "        input_height_width=28,\n",
    "    )\n",
    "    nnet.train(\n",
    "        Xtrain=Xtrain,\n",
    "        Ttrain=Ttrain,\n",
    "        Xtest=Xtest,\n",
    "        Ttest=Ttest,\n",
    "        n_iterations=10,\n",
    "        batch_size=100,\n",
    "        learning_rate=0.001,\n",
    "    )\n",
    "    return nnet\n",
    "\n",
    "windows_values = [[5, 7]]#, [3, 4], [15, 10]]\n",
    "strides_values = [[1, 2], [2, 2]]#, [1, 3]]\n",
    "wind_strid_config = []\n",
    "wind_strid_accuracy = []\n",
    "for windows in windows_values:\n",
    "    for strides in strides_values:\n",
    "        print('(windows, strides):', (windows, strides))\n",
    "        nnet = run21(windows, strides)\n",
    "        Ytest, _, _ = nnet.use(Xtest)\n",
    "        #Ytest = Ttest # DEBUG\n",
    "        num_correct = np.sum(Ytest == Ttest)\n",
    "        wind_strid_config.append((windows, strides))\n",
    "        wind_strid_accuracy.append(num_correct / Ytest.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Section 2, Part 1, Test Percentage Correct vs Windows and Strides')\n",
    "plt.xlabel('(Windows, Strides)')\n",
    "plt.ylabel('Test Percentage Correct')\n",
    "plt.xticks(range(len(wind_strid_config)), wind_strid_config, rotation=30, horizontalalignment='right')\n",
    "plt.plot(wind_strid_accuracy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = np.argmax(wind_strid_accuracy)\n",
    "best_windows, best_strides = wind_strid_config[bi]\n",
    "print('best windows:', best_windows)\n",
    "print('best strides:', best_strides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run22(n_hiddens_by_layer, n_iterations, n_conv_layers, windows, strides):\n",
    "    nnet = NeuralNetworkClassifier_Pytorch(\n",
    "        n_inputs=1,\n",
    "        n_hiddens_by_layer=n_hiddens_by_layer,\n",
    "        n_outputs=10,\n",
    "        #relu=,\n",
    "        #gpu=,\n",
    "        n_conv_layers=n_conv_layers,\n",
    "        windows=windows,\n",
    "        strides=strides,\n",
    "        input_height_width=28,\n",
    "    )\n",
    "    nnet.train(\n",
    "        Xtrain=Xtrain,\n",
    "        Ttrain=Ttrain,\n",
    "        Xtest=Xtest,\n",
    "        Ttest=Ttest,\n",
    "        n_iterations=n_iterations,\n",
    "        batch_size=100,\n",
    "        learning_rate=0.001,\n",
    "    )\n",
    "    return nnet\n",
    "\n",
    "n_hiddens_by_layer_values = [[50, 20, 10], [100, 20], [10, 10, 10, 10]]\n",
    "n_iterations_values = [25, 3, 15]\n",
    "n_conv_layers_values = [1, 1, 4]\n",
    "windows_values = [[10], [25], [4, 2, 2, 2]]\n",
    "strides_values = [[2], [1], [1, 2, 1, 1]]\n",
    "\n",
    "pairs = list(zip(n_hiddens_by_layer_values, n_iterations_values, n_conv_layers_values, windows_values, strides_values))\n",
    "full_accuracy = []\n",
    "for pair in pairs:\n",
    "\n",
    "    print('(n_hiddens_by_layer, n_iterations, n_conv_layers, windows, strides):', pair)\n",
    "    nnet = run22(*pair)\n",
    "    Ytest, _, _ = nnet.use(Xtest)\n",
    "    #Ytest = Ttest  # DEBUG\n",
    "    num_correct = np.sum(Ytest == Ttest)\n",
    "    full_accuracy.append(num_correct / Ytest.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Section 2, Part 1, Test Percentage Correct vs Windows and Strides')\n",
    "plt.xlabel('(n_hiddens_by_layer, n_iterations, n_conv_layers, windows, strides)')\n",
    "plt.ylabel('Test Percentage Correct')\n",
    "plt.xticks(range(len(pairs)), pairs, rotation=30, horizontalalignment='right')\n",
    "plt.plot(full_accuracy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = np.argmax(full_accuracy)\n",
    "best_pair = pairs[bi]\n",
    "print('best combination:')\n",
    "names = (\"n_hiddens_by_layer\", \"n_iterations\", \"n_conv_layers\", \"windows\", \"strides\")\n",
    "for name, param  in zip(names, pair):\n",
    "    print(name+\":\", param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Graph analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Section 3: Cost and Acc (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output from the `train` function we see two values, one called `cost` and one called `acc`.  What is the meaning of each and why are their\n",
    "values so different?  Study the code to help you answer this question.\n",
    "\n",
    "**ANSWER:**  (20 points) \n",
    "The algorithm is trying to minimize an objective function. The value of this objective function is called the cost,\n",
    "so from the training algorithm. Here are all the important lines:\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    for i in range(n_iterations):\n",
    "        cost = 0.\n",
    "        num_batches = n_examples // batch_size\n",
    "        for k in range(num_batches):\n",
    "            output = loss.forward(Y, Ttrain_batch)\n",
    "            cost += output.item()\n",
    "        if (i + 1) % print_every == 0:\n",
    "            print('Iteration {:d}, cost = {:.4f}, acc = {:.2f}'.format(i + 1, cost / num_batches, 100. * np.mean(classes == Ttest)))\n",
    "\n",
    "The cost is reset for every iteration, and the cost is calculated for some number of batches and accumulated in the\n",
    "cost variable. Therefore, the cost number represents the average cost per batch for that iteration.\n",
    "\n",
    "The cross entropy is calculated by the following formula (according to wikipedia):\n",
    "\n",
    "$$\n",
    "H(p, q) &= - \\sum_{x \\in X} p(x) \\log q(x)\n",
    "$$\n",
    "\n",
    "where $x$ is the class, $X$ is the set of possible classes, $p$ is the actual probability distribution, and $q$ is the model's approximation of this distribution, and $H$ is the loss. The model's approximation of the distribution is the softmax of its output matrix, and this is calculated for each prediction. Since we have the true labels and know what it should be, $p$ is a boring distribution which always equals 1 for the class of the data point and 0 otherwise. Therefore, our equation simplifies to  \n",
    "\n",
    "$$\n",
    "H(p, q) = - \\log q(x)\n",
    "$$\n",
    "\n",
    "where $x$ is the true class of the data point. $-\\log q$ is a decreasing function with $q$. $q$ increases as the model becomes more accurate (predicts high probabilities for the true class). Since $q$ is a probability distribution, it has a domain of $[0, 1]$, and $\\log [0, 1] = (-\\infty, 0]$. Therefore, as the model becomes more accurate, $q$ increases, and the $\\log$ approaches zero. Thus, the loss also converges to zero when summed over a set of predictions, i.e., in the iteration. \n",
    "\n",
    "Cross entropy has the added bonus of taking the confidence of the model into account. Even if the model predicts correctly, if it's probability was 50%, we could still have a loss greater than 0 since it should be 100%. This might suggest that loss is a better measurement of innacuracy (see below). Below, I found that loss is still high even though accuracy can still be strong.\n",
    "\n",
    "The accuracy is calculated from this piece of code: `100. * np.mean(classes == Ttest)`. `classes` represents the \n",
    "class predictions after the iteration. `classes == Ttest` will return an array of booleans. `np.mean` will count the \n",
    "`True`'s (since they equal 1 and it sums the `True`'s) and divide them by the number of predictions in the class \n",
    "resulting in the percent accuracy (in decimal form). It's multiplied by 100 to convert it to percentage of correct \n",
    "predictions.\n",
    "\n",
    "In summary, while both loss and accuracy represent the same thing, the correctness of the model, loss is what we are trying to optimize and roughly measures the models incorrectness (lower is better), and accuracy is a measure of accuracy (how often it is correct, higher is better). Also, loss could still be non zero if the model predicts perfectly since it takes into account when the model isn't 100% confident. \n",
    "\n",
    "Everything below is some experimenting I did because I was curious. I wanted to show that cost can be high if the model is just guessing the most common class. You don't have to read it since it's not really part of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "split = .95\n",
    "test_train_split = .7\n",
    "test_train_splitN = int(N * test_train_split)\n",
    "splitN = int(N*split)\n",
    "XTest = np.random.rand(N, 4) # already random so no need to shuffle\n",
    "TTest = np.ones(N)\n",
    "TTest[splitN:] = 0\n",
    "np.random.shuffle(TTest)\n",
    "\n",
    "XtrainTest=XTest[:test_train_splitN, None, :].reshape(-1, 1, 2, 2)\n",
    "TtrainTest=TTest[:test_train_splitN]\n",
    "XtestTest=XTest[test_train_splitN:, None, :].reshape(-1, 1, 2, 2)\n",
    "TtestTest=TTest[test_train_splitN:]\n",
    "# This represents a random distribution of 95% class 1 and 5% class 0. p(1) = .95 and p(0) = .5\n",
    "\n",
    "nnet = NeuralNetworkClassifier_Pytorch(\n",
    "    n_inputs=4,\n",
    "    n_hiddens_by_layer=[10, 10],\n",
    "    n_outputs=2\n",
    ")\n",
    "nnet.train(\n",
    "    Xtrain=XtrainTest,\n",
    "    Ttrain=TtrainTest,\n",
    "    Xtest=XtestTest,\n",
    "    Ttest=TtestTest,\n",
    "    n_iterations=100,\n",
    "    batch_size=100,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "def calc_classes_from_prob(prob):\n",
    "    prob, _ = prob # get prob of 0, should be near 0.05\n",
    "    if np.random.rand() > prob:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "print(\"shape of X and T test\")\n",
    "print(XtestTest.shape, TtestTest.shape)\n",
    "classes, probs, _ = nnet.use(XtestTest)\n",
    "real_classes = [calc_classes_from_prob(prob) for prob in probs]\n",
    "print('distribution of q (model) for testing set')\n",
    "q = np.mean(probs, axis=0)\n",
    "print(q)\n",
    "print('distribution of p (actual) for testing set')\n",
    "p = np.array([1- np.mean(TtestTest), np.mean(TtestTest)])\n",
    "print(p)\n",
    "print('distribution ')\n",
    "print('accuracy using max prob:', np.mean(classes==TtestTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we can see that the accuracy is really high 95.7%, and the model's approximation of the distribution is right, but the cost is really high compared to values for actually correlated data (e.g., 0.0xxx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sample from the distribution to select our class (as opposed to just choosing the maximum, we can see our accuracy drop. This is more interesting than relavent to this discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "n=100\n",
    "for _ in range(n):\n",
    "    real_classes = [calc_classes_from_prob(prob) for prob in probs]\n",
    "    acc = np.mean(real_classes == TtestTest)\n",
    "    accs.append(acc)\n",
    "\n",
    "print('mean accuracy sampling from q (n={}): {}'.format(n, np.mean(accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the actual cost of the testing set, we have to do it manually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(p, q) &= - \\sum_{x \\in X} p(x) \\log q(x) \\\\\n",
    "&= -\\log(\\prod_{x \\in X}q(x)^{p(x)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "in the cost calculation p(x) = 1 if $x$ is the right class, so\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(p, q) &= -\\log(\\prod_{x \\in X}q(x)^{p(x)}) \\\\\n",
    "&= -\\log(q(x))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $x$ is the correct class. This can be used to calculate the cost of the testing set `Ttest` given `probs` from\n",
    "`nnet.use(Xtest)`. (This assumes Ttest's value is the index of the class in probs; easily\n",
    "the case, but worth noting).\n",
    "\n",
    "    cost = np.mean(-np.log(probs[:, Ttest.astype(int)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = np.mean(np.sum(-p * np.log(probs), axis=1))\n",
    "obj_cost = np.mean(-np.log(probs[:, TtestTest.astype(int)]))\n",
    "print('actual cost:', cost)\n",
    "print('objective function cost:', obj_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something I wasn't expecting, the values are nearly identical. Not sure why this is.. One thing we do know is that the cost is high despite the accuracy being really good.\n",
    "\n",
    "Something I'm wondering is that the high cost despite perfectly capturing the distribution is due to the fact\n",
    "that T is a random variable as opposed to having p(x) = 1 given X. I'm not a statisititian, so I don't know how to figure this out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare performance and training times for some of the parameter variations used in your main report when run on a GPU versus a CPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
